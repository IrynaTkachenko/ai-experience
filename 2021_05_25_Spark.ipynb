{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_05_25_Spark.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrynaTkachenko/ai-experience/blob/main/2021_05_25_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "# Oснови Spark\n",
        "\n",
        "Для запуску в середовищі Google Colab запустимо наступний код"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0cda99-b5fe-49ac-ca64-87b1dca16784"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://apache.volia.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-25 11:57:40--  https://apache.volia.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
            "Resolving apache.volia.net (apache.volia.net)... 82.144.192.7\n",
            "Connecting to apache.volia.net (apache.volia.net)|82.144.192.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224374704 (214M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.1-bin-hadoop2.7.tgz.1’\n",
            "\n",
            "spark-3.1.1-bin-had 100%[===================>] 213.98M  63.0MB/s    in 3.6s    \n",
            "\n",
            "2021-05-25 11:57:44 (59.2 MB/s) - ‘spark-3.1.1-bin-hadoop2.7.tgz.1’ saved [224374704/224374704]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCItMgNZRJtE"
      },
      "source": [
        "!tar xf '/content/spark-3.1.1-bin-hadoop2.7.tgz'\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Spark та Java у Colab встановлено.\n",
        "Встановимо шляхи до середовища, які дозволятимуть запускати Pyspark. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Запустимо локальну сесію Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Вітаємо! Colab вже працює з Pyspark. \n",
        "Побудуємо просту модель лінійної регресії.\n",
        "\n",
        "# Модель лінійної регресії\n",
        "\n",
        "\n",
        "Модель лінійної регресії є одним із найдавніших та широко застосовуваних підходів до машинного навчання, який передбачає взаємозв'язок між залежними та незалежними змінними. \n",
        "\n",
        "Лінійна регресія складається з лінії, що найкраще підходить, через розрізнені точки на графіку, а лінія, що найкраще підходить, відома як лінія регресії.\n",
        "\n",
        "Мета цієї вправи прогнозувати ціни на житло за заданими ознаками. Давайте спрогнозуємо ціни набору даних про житло в Бостоні, розглядаючи MEDV як вихідну змінну, а всі інші змінні як вхідні."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0454bd8f-da41-4516-a3ec-99f37446cefa"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/asifahmed90/pyspark-ML-in-Colab/master/BostonHousing.csv\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-25 11:57:56--  https://raw.githubusercontent.com/asifahmed90/pyspark-ML-in-Colab/master/BostonHousing.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35735 (35K) [text/plain]\n",
            "Saving to: ‘BostonHousing.csv.2’\n",
            "\n",
            "BostonHousing.csv.2 100%[===================>]  34.90K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-05-25 11:57:56 (24.7 MB/s) - ‘BostonHousing.csv.2’ saved [35735/35735]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg"
      },
      "source": [
        "Перевіряємо наявність файлів"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2fac5a-239f-4d3e-e18d-70d2441b5199"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BostonHousing.csv\t   spark-3.1.1-bin-hadoop2.7.tgz\n",
            "BostonHousing.csv.1\t   spark-3.1.1-bin-hadoop2.7.tgz.1\n",
            "BostonHousing.csv.2\t   sparkjob.py\n",
            "sample_data\t\t   testsparkjob.py\n",
            "spark-3.1.1-bin-hadoop2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Тепер, коли ми завантажили набір даних, ми можемо розпочати аналіз.\n",
        "Для нашої моделі лінійної регресії нам потрібно імпортувати два модулі з Pyspark, тобто Vector Assembler та Linear Regression. Vector Assembler - це трансформатор, який збирає всі функції в один вектор із декількох стовпців, що містять тип float/double. Ми могли б використовувати StringIndexer, якщо будь-який із наших стовпців містить значення рядків для перетворення його в числові значення. На щастя, набір даних BostonHousing містить лише подвійні значення, тому наразі нам не потрібно турбуватися про StringIndexer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import StructType, IntegerType, DateType, StringType, FloatType\n",
        "\n",
        "#'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'\n",
        "schema = StructType([\n",
        "    StructField(\"crim\", FloatType()),\n",
        "    StructField(\"zn\", FloatType()),\n",
        "    StructField(\"indus\", FloatType()),\n",
        "    StructField(\"chas\", FloatType()),\n",
        "    StructField(\"nox\", FloatType()),\n",
        "    StructField(\"rm\", FloatType()),\n",
        "    StructField(\"age\", FloatType()),\n",
        "    StructField(\"dis\", FloatType()),\n",
        "    StructField(\"rad\", FloatType()),\n",
        "    StructField(\"tax\", FloatType()),\n",
        "    StructField(\"ptratio\", FloatType()),\n",
        "    StructField(\"b\", FloatType()),\n",
        "    StructField(\"lstat\", FloatType()),\n",
        "    StructField(\"medv\", FloatType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.csv('BostonHousing.csv', schema=schema, header =True, \n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzHsmSOVXViW"
      },
      "source": [
        "# Або\n",
        "dataset = spark.read.csv('BostonHousing.csv', inferSchema=True, header =True, \n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnTARPleoCVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e91afcd-f0ec-45f3-ebf0-1f0f01d9b271"
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.538, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, b=396.9, lstat=4.98, medv=24.0),\n",
              " Row(crim=0.02731, zn=0.0, indus=7.07, chas=0, nox=0.469, rm=6.421, age=78.9, dis=4.9671, rad=2, tax=242, ptratio=17.8, b=396.9, lstat=9.14, medv=21.6),\n",
              " Row(crim=0.02729, zn=0.0, indus=7.07, chas=0, nox=0.469, rm=7.185, age=61.1, dis=4.9671, rad=2, tax=242, ptratio=17.8, b=392.83, lstat=4.03, medv=34.7),\n",
              " Row(crim=0.03237, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=6.998, age=45.8, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=394.63, lstat=2.94, medv=33.4),\n",
              " Row(crim=0.06905, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=7.147, age=54.2, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=396.9, lstat=5.33, medv=36.2),\n",
              " Row(crim=0.02985, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=6.43, age=58.7, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=394.12, lstat=5.21, medv=28.7),\n",
              " Row(crim=0.08829, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.012, age=66.6, dis=5.5605, rad=5, tax=311, ptratio=15.2, b=395.6, lstat=12.43, medv=22.9),\n",
              " Row(crim=0.14455, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.172, age=96.1, dis=5.9505, rad=5, tax=311, ptratio=15.2, b=396.9, lstat=19.15, medv=27.1),\n",
              " Row(crim=0.21124, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=5.631, age=100.0, dis=6.0821, rad=5, tax=311, ptratio=15.2, b=386.63, lstat=29.93, medv=16.5),\n",
              " Row(crim=0.17004, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.004, age=85.9, dis=6.5921, rad=5, tax=311, ptratio=15.2, b=386.71, lstat=17.1, medv=18.9)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLoAfqVv8-E"
      },
      "source": [
        "Зверніть увагу, що ми використовували InferSchema всередині модуля read.csv. InferSchema дозволяє нам автоматично виводити різні типи даних для кожного стовпця.\n",
        "\n",
        "Давайте роздрукуємося в наборі даних, щоб побачити типи даних кожного стовпця:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gok1FXWugYkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f25cf60-8ac2-47ea-cec7-62ed62d32e00"
      },
      "source": [
        "dataset.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- crim: double (nullable = true)\n",
            " |-- zn: double (nullable = true)\n",
            " |-- indus: double (nullable = true)\n",
            " |-- chas: integer (nullable = true)\n",
            " |-- nox: double (nullable = true)\n",
            " |-- rm: double (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- dis: double (nullable = true)\n",
            " |-- rad: integer (nullable = true)\n",
            " |-- tax: integer (nullable = true)\n",
            " |-- ptratio: double (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- lstat: double (nullable = true)\n",
            " |-- medv: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9VJqsHwEGf"
      },
      "source": [
        "Наступним кроком є перетворення всіх функцій з різних стовпців в один стовпець і давайте назвемо цей новий векторний стовпець як 'Attributes' у outputCol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSqdT9QgkfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fbae52-d622-4a0f-94d9-eceec989def3"
      },
      "source": [
        "#Input all the features in one vector column\n",
        "assembler = VectorAssembler(inputCols=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'], outputCol = 'Attributes')\n",
        "output = assembler.transform(dataset)\n",
        "#Input vs Output\n",
        "finalized_data = output.select(\"Attributes\",\"medv\")\n",
        "\n",
        "#print(finalized_data)\n",
        "finalized_data.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+\n",
            "|          Attributes|medv|\n",
            "+--------------------+----+\n",
            "|[0.00632,18.0,2.3...|24.0|\n",
            "|[0.02731,0.0,7.07...|21.6|\n",
            "|[0.02729,0.0,7.07...|34.7|\n",
            "|[0.03237,0.0,2.18...|33.4|\n",
            "|[0.06905,0.0,2.18...|36.2|\n",
            "|[0.02985,0.0,2.18...|28.7|\n",
            "|[0.08829,12.5,7.8...|22.9|\n",
            "|[0.14455,12.5,7.8...|27.1|\n",
            "|[0.21124,12.5,7.8...|16.5|\n",
            "|[0.17004,12.5,7.8...|18.9|\n",
            "|[0.22489,12.5,7.8...|15.0|\n",
            "|[0.11747,12.5,7.8...|18.9|\n",
            "|[0.09378,12.5,7.8...|21.7|\n",
            "|[0.62976,0.0,8.14...|20.4|\n",
            "|[0.63796,0.0,8.14...|18.2|\n",
            "|[0.62739,0.0,8.14...|19.9|\n",
            "|[1.05393,0.0,8.14...|23.1|\n",
            "|[0.7842,0.0,8.14,...|17.5|\n",
            "|[0.80271,0.0,8.14...|20.2|\n",
            "|[0.7258,0.0,8.14,...|18.2|\n",
            "+--------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgFCto2wHLd"
      },
      "source": [
        "Тут \"Attributes\" містяться у вхідних функціях з усіх стовпців, а \"medv\" - цільовий стовпець.\n",
        "Далі слід розділити дані навчання та тестування відповідно до нашого набору даних (у цьому випадку 0,8 та 0,2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwe1VT0UNOIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853039e8-5a50-4682-f25b-e28e143b90a9"
      },
      "source": [
        "#Split training and testing data\n",
        "train_data,test_data = finalized_data.randomSplit([0.8,0.2])\n",
        "\n",
        "\n",
        "regressor = LinearRegression(featuresCol = 'Attributes', labelCol = 'medv')\n",
        "\n",
        "#Learn to fit the model from training set\n",
        "regressor = regressor.fit(train_data)\n",
        "\n",
        "#To predict the prices on testing set\n",
        "pred = regressor.evaluate(test_data)\n",
        "\n",
        "#Predict the model\n",
        "pred.predictions.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+------------------+\n",
            "|          Attributes|medv|        prediction|\n",
            "+--------------------+----+------------------+\n",
            "|[0.01965,80.0,1.7...|20.1|21.119097258075737|\n",
            "|[0.02177,82.5,2.0...|42.3|36.236488681553844|\n",
            "|[0.02899,40.0,1.2...|26.6|21.995687844596937|\n",
            "|[0.02985,0.0,2.18...|28.7|24.979849071675442|\n",
            "|[0.03427,0.0,5.19...|19.5|20.262686886665314|\n",
            "|[0.03551,25.0,4.8...|22.9| 25.34422557691682|\n",
            "|[0.04337,21.0,5.6...|20.5|24.286238516525565|\n",
            "|[0.04379,80.0,3.3...|19.4|26.602795965572252|\n",
            "|[0.04462,25.0,4.8...|23.9| 27.01555398994781|\n",
            "|[0.04741,0.0,11.9...|11.9|22.478464711398303|\n",
            "|[0.05479,33.0,2.1...|28.4|30.443380234699443|\n",
            "|[0.05515,33.0,2.1...|36.1| 32.23127950179518|\n",
            "|[0.05602,0.0,2.46...|50.0| 33.98917462577596|\n",
            "|[0.05644,40.0,6.4...|32.4| 36.07040547764153|\n",
            "|[0.05646,0.0,12.8...|21.2| 21.37970698136602|\n",
            "|[0.0566,0.0,3.41,...|23.6|29.818463777893434|\n",
            "|[0.0578,0.0,2.46,...|37.2| 31.81980072869767|\n",
            "|[0.06127,40.0,6.4...|33.1| 34.75369118453489|\n",
            "|[0.06129,20.0,3.3...|46.0| 39.23926308237282|\n",
            "|[0.06211,40.0,1.2...|22.9| 20.77894925493835|\n",
            "+--------------------+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "Ми також можемо надрукувати коефіцієнт і перехоплення моделі регресії, використовуючи таку команду:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76df5c24-648f-41eb-e6fb-ae8c574408f3"
      },
      "source": [
        "#coefficient of the regression model\n",
        "coeff = regressor.coefficients\n",
        "\n",
        "#X and Y intercept\n",
        "intr = regressor.intercept\n",
        "\n",
        "print (\"The coefficient of the model is : %a\" %coeff)\n",
        "print (\"The Intercept of the model is : %f\" %intr)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of the model is : DenseVector([-0.1025, 0.0575, 0.041, 3.0304, -20.1239, 2.882, 0.0076, -1.4193, 0.2981, -0.012, -0.8926, 0.0075, -0.5264])\n",
            "The Intercept of the model is : 41.984725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYORz3Q9wTSW"
      },
      "source": [
        "# Iнше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qrQdEj62ptt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d04336b-76a0-4e49-e653-670e8feb274b"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "eval = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Root Mean Square Error\n",
        "rmse = eval.evaluate(pred.predictions)\n",
        "print(\"RMSE: %.3f\" % rmse)\n",
        "\n",
        "# Mean Square Error\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "print(\"MSE: %.3f\" % mse)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "print(\"MAE: %.3f\" % mae)\n",
        "\n",
        "# r2 - coefficient of determination\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
        "print(\"r2: %.3f\" %r2)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 5.325\n",
            "MSE: 28.359\n",
            "MAE: 3.850\n",
            "r2: 0.777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXxb9RNyTfPv"
      },
      "source": [
        "#Створення Spark служби"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeP-sGnYRr-D"
      },
      "source": [
        "Згенеруємо службу та запустимо її"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orverCFWVwB7"
      },
      "source": [
        "sparkjob =  '''\n",
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        " \n",
        "  # create Spark context with Spark configuration\n",
        "  conf = SparkConf().setAppName(\"Read Text to RDD - Python\")\n",
        "  sc = SparkContext(conf=conf)\n",
        " \n",
        "  # read input text file to RDD\n",
        "  numbers = sc.parallelize([1,7,8,9,5,77,48])\n",
        " \n",
        "  # aggregate RDD elements using addition function\n",
        "  sum = numbers.reduce(lambda a,b:a+b)\n",
        " \n",
        "  print (\"sum is : \" + str(sum))\n",
        "'''\n",
        "\n",
        "f = open('testsparkjob.py', 'w')\n",
        "f.write(sparkjob)\n",
        "f.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcIpcpVQRbG_",
        "outputId": "585864eb-d6c7-411f-f765-e3447a3a63d9"
      },
      "source": [
        "!/content/spark-3.1.1-bin-hadoop2.7/bin/spark-submit /content/testsparkjob.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/05/25 11:58:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/05/25 11:58:11 INFO SparkContext: Running Spark version 3.1.1\n",
            "21/05/25 11:58:11 INFO ResourceUtils: ==============================================================\n",
            "21/05/25 11:58:11 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/05/25 11:58:11 INFO ResourceUtils: ==============================================================\n",
            "21/05/25 11:58:11 INFO SparkContext: Submitted application: Read Text to RDD - Python\n",
            "21/05/25 11:58:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/05/25 11:58:11 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/05/25 11:58:11 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/05/25 11:58:11 INFO SecurityManager: Changing view acls to: root\n",
            "21/05/25 11:58:11 INFO SecurityManager: Changing modify acls to: root\n",
            "21/05/25 11:58:11 INFO SecurityManager: Changing view acls groups to: \n",
            "21/05/25 11:58:11 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/05/25 11:58:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/05/25 11:58:11 INFO Utils: Successfully started service 'sparkDriver' on port 35751.\n",
            "21/05/25 11:58:11 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/05/25 11:58:11 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/05/25 11:58:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/05/25 11:58:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/05/25 11:58:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/05/25 11:58:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a8fabe67-c46f-49de-8863-55a1cab0199e\n",
            "21/05/25 11:58:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "21/05/25 11:58:11 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/05/25 11:58:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "21/05/25 11:58:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "21/05/25 11:58:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://8300a878246c:4041\n",
            "21/05/25 11:58:12 INFO Executor: Starting executor ID driver on host 8300a878246c\n",
            "21/05/25 11:58:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39359.\n",
            "21/05/25 11:58:12 INFO NettyBlockTransferService: Server created on 8300a878246c:39359\n",
            "21/05/25 11:58:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/05/25 11:58:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8300a878246c, 39359, None)\n",
            "21/05/25 11:58:12 INFO BlockManagerMasterEndpoint: Registering block manager 8300a878246c:39359 with 366.3 MiB RAM, BlockManagerId(driver, 8300a878246c, 39359, None)\n",
            "21/05/25 11:58:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8300a878246c, 39359, None)\n",
            "21/05/25 11:58:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8300a878246c, 39359, None)\n",
            "21/05/25 11:58:13 INFO SparkContext: Starting job: reduce at /content/testsparkjob.py:16\n",
            "21/05/25 11:58:13 INFO DAGScheduler: Got job 0 (reduce at /content/testsparkjob.py:16) with 2 output partitions\n",
            "21/05/25 11:58:13 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /content/testsparkjob.py:16)\n",
            "21/05/25 11:58:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "21/05/25 11:58:13 INFO DAGScheduler: Missing parents: List()\n",
            "21/05/25 11:58:13 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /content/testsparkjob.py:16), which has no missing parents\n",
            "21/05/25 11:58:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.3 KiB, free 366.3 MiB)\n",
            "21/05/25 11:58:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 366.3 MiB)\n",
            "21/05/25 11:58:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8300a878246c:39359 (size: 3.5 KiB, free: 366.3 MiB)\n",
            "21/05/25 11:58:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383\n",
            "21/05/25 11:58:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /content/testsparkjob.py:16) (first 15 tasks are for partitions Vector(0, 1))\n",
            "21/05/25 11:58:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "21/05/25 11:58:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8300a878246c, executor driver, partition 0, PROCESS_LOCAL, 4493 bytes) taskResourceAssignments Map()\n",
            "21/05/25 11:58:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (8300a878246c, executor driver, partition 1, PROCESS_LOCAL, 4520 bytes) taskResourceAssignments Map()\n",
            "21/05/25 11:58:13 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "21/05/25 11:58:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "21/05/25 11:58:15 INFO PythonRunner: Times: total = 514, boot = 489, init = 24, finish = 1\n",
            "21/05/25 11:58:15 INFO PythonRunner: Times: total = 556, boot = 525, init = 30, finish = 1\n",
            "21/05/25 11:58:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1354 bytes result sent to driver\n",
            "21/05/25 11:58:15 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1354 bytes result sent to driver\n",
            "21/05/25 11:58:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1447 ms on 8300a878246c (executor driver) (1/2)\n",
            "21/05/25 11:58:15 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 38205\n",
            "21/05/25 11:58:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1419 ms on 8300a878246c (executor driver) (2/2)\n",
            "21/05/25 11:58:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "21/05/25 11:58:15 INFO DAGScheduler: ResultStage 0 (reduce at /content/testsparkjob.py:16) finished in 1.790 s\n",
            "21/05/25 11:58:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "21/05/25 11:58:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "21/05/25 11:58:15 INFO DAGScheduler: Job 0 finished: reduce at /content/testsparkjob.py:16, took 1.887548 s\n",
            "sum is : 155\n",
            "21/05/25 11:58:15 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "21/05/25 11:58:15 INFO SparkUI: Stopped Spark web UI at http://8300a878246c:4041\n",
            "21/05/25 11:58:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "21/05/25 11:58:15 INFO MemoryStore: MemoryStore cleared\n",
            "21/05/25 11:58:15 INFO BlockManager: BlockManager stopped\n",
            "21/05/25 11:58:15 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "21/05/25 11:58:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "21/05/25 11:58:15 INFO SparkContext: Successfully stopped SparkContext\n",
            "21/05/25 11:58:15 INFO ShutdownHookManager: Shutdown hook called\n",
            "21/05/25 11:58:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff55d3e4-8b6e-473f-a5e0-f285195473fa\n",
            "21/05/25 11:58:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff55d3e4-8b6e-473f-a5e0-f285195473fa/pyspark-dfdd48e9-e114-42b9-abd1-949cc4725962\n",
            "21/05/25 11:58:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-943013d1-b260-4d2d-850a-0a6f929ae00c\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}