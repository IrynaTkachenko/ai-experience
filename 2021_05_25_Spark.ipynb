{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_05_25_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrynaTkachenko/ai-experience/blob/main/2021_05_25_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "# Oснови Spark\n",
        "\n",
        "Для запуску в середовищі Google Colab запустимо наступний код"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade90e34-119a-4f80-f518-2af544d6cbe5"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://apache.volia.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 11:47:26--  https://apache.volia.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
            "Resolving apache.volia.net (apache.volia.net)... 82.144.192.7\n",
            "Connecting to apache.volia.net (apache.volia.net)|82.144.192.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224374704 (214M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.1-bin-hadoop2.7.tgz.1’\n",
            "\n",
            "spark-3.1.1-bin-had 100%[===================>] 213.98M  17.6MB/s    in 14s     \n",
            "\n",
            "2021-05-26 11:47:41 (15.5 MB/s) - ‘spark-3.1.1-bin-hadoop2.7.tgz.1’ saved [224374704/224374704]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCItMgNZRJtE"
      },
      "source": [
        "!tar xf '/content/spark-3.1.1-bin-hadoop2.7.tgz'\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Spark та Java у Colab встановлено.\n",
        "Встановимо шляхи до середовища, які дозволятимуть запускати Pyspark. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Запустимо локальну сесію Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkfvbP72BO24"
      },
      "source": [
        "Вітаємо! Colab вже працює з Pyspark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BldrdnAIBROK"
      },
      "source": [
        "#Spark Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X48FGLdxZq9G",
        "outputId": "1791d34a-4abd-4d4f-aea5-d2c73124766c"
      },
      "source": [
        "valuesA = [('Pirate',1),('Monkey',2),('Ninja',3),('Spaghetti',4)]\n",
        "TableA = spark.createDataFrame(valuesA,['name','id'])\n",
        " \n",
        "valuesB = [('Rutabaga',1),('Pirate',2),('Ninja',3),('Darth Vader',4)]\n",
        "TableB = spark.createDataFrame(valuesB,['name','id'])\n",
        " \n",
        "TableA.show()\n",
        "TableB.show()\n",
        "\n",
        "ta = TableA.alias('ta')\n",
        "tb = TableB.alias('tb')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---+\n",
            "|     name| id|\n",
            "+---------+---+\n",
            "|   Pirate|  1|\n",
            "|   Monkey|  2|\n",
            "|    Ninja|  3|\n",
            "|Spaghetti|  4|\n",
            "+---------+---+\n",
            "\n",
            "+-----------+---+\n",
            "|       name| id|\n",
            "+-----------+---+\n",
            "|   Rutabaga|  1|\n",
            "|     Pirate|  2|\n",
            "|      Ninja|  3|\n",
            "|Darth Vader|  4|\n",
            "+-----------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcEqL20wCXuo"
      },
      "source": [
        "#Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYaifjh_aXQR",
        "outputId": "a4a54f98-bf8a-4140-b94d-12e5bedc05d6"
      },
      "source": [
        "inner_join = ta.join(tb, ta.name == tb.name)\n",
        "inner_join.show()\n",
        "\n",
        "left_join = ta.join(tb, ta.name == tb.name,how='left') \n",
        "left_join.show()\n",
        "\n",
        "right_join = ta.join(tb, ta.name == tb.name,how='right')\n",
        "right_join.show()\n",
        "\n",
        "full_outer_join = ta.join(tb, ta.name == tb.name,how='full')\n",
        "full_outer_join.show()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+------+---+\n",
            "|  name| id|  name| id|\n",
            "+------+---+------+---+\n",
            "| Ninja|  3| Ninja|  3|\n",
            "|Pirate|  1|Pirate|  2|\n",
            "+------+---+------+---+\n",
            "\n",
            "+---------+---+------+----+\n",
            "|     name| id|  name|  id|\n",
            "+---------+---+------+----+\n",
            "|Spaghetti|  4|  null|null|\n",
            "|    Ninja|  3| Ninja|   3|\n",
            "|   Pirate|  1|Pirate|   2|\n",
            "|   Monkey|  2|  null|null|\n",
            "+---------+---+------+----+\n",
            "\n",
            "+------+----+-----------+---+\n",
            "|  name|  id|       name| id|\n",
            "+------+----+-----------+---+\n",
            "|  null|null|   Rutabaga|  1|\n",
            "| Ninja|   3|      Ninja|  3|\n",
            "|Pirate|   1|     Pirate|  2|\n",
            "|  null|null|Darth Vader|  4|\n",
            "+------+----+-----------+---+\n",
            "\n",
            "+---------+----+-----------+----+\n",
            "|     name|  id|       name|  id|\n",
            "+---------+----+-----------+----+\n",
            "|     null|null|   Rutabaga|   1|\n",
            "|Spaghetti|   4|       null|null|\n",
            "|    Ninja|   3|      Ninja|   3|\n",
            "|   Pirate|   1|     Pirate|   2|\n",
            "|   Monkey|   2|       null|null|\n",
            "|     null|null|Darth Vader|   4|\n",
            "+---------+----+-----------+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "# Spark ML. Модель лінійної регресії\n",
        "\n",
        "\n",
        "Модель лінійної регресії є одним із найдавніших та широко застосовуваних підходів до машинного навчання, який передбачає взаємозв'язок між залежними та незалежними змінними. \n",
        "\n",
        "Лінійна регресія складається з лінії, що найкраще підходить, через розрізнені точки на графіку, а лінія, що найкраще підходить, відома як лінія регресії.\n",
        "\n",
        "Мета цієї вправи прогнозувати ціни на житло за заданими ознаками. Давайте спрогнозуємо ціни набору даних про житло в Бостоні, розглядаючи MEDV як вихідну змінну, а всі інші змінні [текст посилання](https://)як вхідні."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a802a0-d91f-4a7f-ac18-c193a76bbaf6"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/asifahmed90/pyspark-ML-in-Colab/master/BostonHousing.csv\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 11:48:07--  https://raw.githubusercontent.com/asifahmed90/pyspark-ML-in-Colab/master/BostonHousing.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35735 (35K) [text/plain]\n",
            "Saving to: ‘BostonHousing.csv.1’\n",
            "\n",
            "BostonHousing.csv.1 100%[===================>]  34.90K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-05-26 11:48:07 (22.5 MB/s) - ‘BostonHousing.csv.1’ saved [35735/35735]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg"
      },
      "source": [
        "Перевіряємо наявність файлів"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cbf43f-ae9d-465f-b17f-81b4e5beea90"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BostonHousing.csv    spark-3.1.1-bin-hadoop2.7\t      testsparkjob.py\n",
            "BostonHousing.csv.1  spark-3.1.1-bin-hadoop2.7.tgz\n",
            "sample_data\t     spark-3.1.1-bin-hadoop2.7.tgz.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Тепер, коли ми завантажили набір даних, ми можемо розпочати аналіз.\n",
        "Для нашої моделі лінійної регресії нам потрібно імпортувати два модулі з Pyspark, тобто Vector Assembler та Linear Regression. Vector Assembler - це трансформатор, який збирає всі функції в один вектор із декількох стовпців, що містять тип float/double. Ми могли б використовувати StringIndexer, якщо будь-який із наших стовпців містить значення рядків для перетворення його в числові значення. На щастя, набір даних BostonHousing містить лише подвійні значення, тому наразі нам не потрібно турбуватися про StringIndexer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import StructType, IntegerType, DateType, StringType, FloatType\n",
        "\n",
        "#'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'\n",
        "schema = StructType([\n",
        "    StructField(\"crim\", FloatType()),\n",
        "    StructField(\"zn\", FloatType()),\n",
        "    StructField(\"indus\", FloatType()),\n",
        "    StructField(\"chas\", FloatType()),\n",
        "    StructField(\"nox\", FloatType()),\n",
        "    StructField(\"rm\", FloatType()),\n",
        "    StructField(\"age\", FloatType()),\n",
        "    StructField(\"dis\", FloatType()),\n",
        "    StructField(\"rad\", FloatType()),\n",
        "    StructField(\"tax\", FloatType()),\n",
        "    StructField(\"ptratio\", FloatType()),\n",
        "    StructField(\"b\", FloatType()),\n",
        "    StructField(\"lstat\", FloatType()),\n",
        "    StructField(\"medv\", FloatType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.csv('BostonHousing.csv', schema=schema, header =True, \n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzHsmSOVXViW"
      },
      "source": [
        "# Або\n",
        "dataset = spark.read.csv('BostonHousing.csv', inferSchema=True, header =True, \n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnTARPleoCVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0d9096-996c-4da4-c3f6-00c3a2e7048f"
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.538, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, b=396.9, lstat=4.98, medv=24.0),\n",
              " Row(crim=0.02731, zn=0.0, indus=7.07, chas=0, nox=0.469, rm=6.421, age=78.9, dis=4.9671, rad=2, tax=242, ptratio=17.8, b=396.9, lstat=9.14, medv=21.6),\n",
              " Row(crim=0.02729, zn=0.0, indus=7.07, chas=0, nox=0.469, rm=7.185, age=61.1, dis=4.9671, rad=2, tax=242, ptratio=17.8, b=392.83, lstat=4.03, medv=34.7),\n",
              " Row(crim=0.03237, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=6.998, age=45.8, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=394.63, lstat=2.94, medv=33.4),\n",
              " Row(crim=0.06905, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=7.147, age=54.2, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=396.9, lstat=5.33, medv=36.2),\n",
              " Row(crim=0.02985, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=6.43, age=58.7, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=394.12, lstat=5.21, medv=28.7),\n",
              " Row(crim=0.08829, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.012, age=66.6, dis=5.5605, rad=5, tax=311, ptratio=15.2, b=395.6, lstat=12.43, medv=22.9),\n",
              " Row(crim=0.14455, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.172, age=96.1, dis=5.9505, rad=5, tax=311, ptratio=15.2, b=396.9, lstat=19.15, medv=27.1),\n",
              " Row(crim=0.21124, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=5.631, age=100.0, dis=6.0821, rad=5, tax=311, ptratio=15.2, b=386.63, lstat=29.93, medv=16.5),\n",
              " Row(crim=0.17004, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.004, age=85.9, dis=6.5921, rad=5, tax=311, ptratio=15.2, b=386.71, lstat=17.1, medv=18.9)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLoAfqVv8-E"
      },
      "source": [
        "Зверніть увагу, що ми використовували InferSchema всередині модуля read.csv. InferSchema дозволяє нам автоматично виводити різні типи даних для кожного стовпця.\n",
        "\n",
        "Давайте роздрукуємося в наборі даних, щоб побачити типи даних кожного стовпця:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gok1FXWugYkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb0d648-ab2d-4ac2-b1cd-33c83a21899e"
      },
      "source": [
        "dataset.printSchema()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- crim: double (nullable = true)\n",
            " |-- zn: double (nullable = true)\n",
            " |-- indus: double (nullable = true)\n",
            " |-- chas: integer (nullable = true)\n",
            " |-- nox: double (nullable = true)\n",
            " |-- rm: double (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- dis: double (nullable = true)\n",
            " |-- rad: integer (nullable = true)\n",
            " |-- tax: integer (nullable = true)\n",
            " |-- ptratio: double (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- lstat: double (nullable = true)\n",
            " |-- medv: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9VJqsHwEGf"
      },
      "source": [
        "Наступним кроком є перетворення всіх функцій з різних стовпців в один стовпець і давайте назвемо цей новий векторний стовпець як 'Attributes' у outputCol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSqdT9QgkfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3134793-d9c3-443f-d9cb-21f3182e3987"
      },
      "source": [
        "#Input all the features in one vector column\n",
        "assembler = VectorAssembler(inputCols=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'], outputCol = 'Attributes')\n",
        "output = assembler.transform(dataset)\n",
        "#Input vs Output\n",
        "finalized_data = output.select(\"Attributes\",\"medv\")\n",
        "\n",
        "#print(finalized_data)\n",
        "finalized_data.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+\n",
            "|          Attributes|medv|\n",
            "+--------------------+----+\n",
            "|[0.00632,18.0,2.3...|24.0|\n",
            "|[0.02731,0.0,7.07...|21.6|\n",
            "|[0.02729,0.0,7.07...|34.7|\n",
            "|[0.03237,0.0,2.18...|33.4|\n",
            "|[0.06905,0.0,2.18...|36.2|\n",
            "|[0.02985,0.0,2.18...|28.7|\n",
            "|[0.08829,12.5,7.8...|22.9|\n",
            "|[0.14455,12.5,7.8...|27.1|\n",
            "|[0.21124,12.5,7.8...|16.5|\n",
            "|[0.17004,12.5,7.8...|18.9|\n",
            "|[0.22489,12.5,7.8...|15.0|\n",
            "|[0.11747,12.5,7.8...|18.9|\n",
            "|[0.09378,12.5,7.8...|21.7|\n",
            "|[0.62976,0.0,8.14...|20.4|\n",
            "|[0.63796,0.0,8.14...|18.2|\n",
            "|[0.62739,0.0,8.14...|19.9|\n",
            "|[1.05393,0.0,8.14...|23.1|\n",
            "|[0.7842,0.0,8.14,...|17.5|\n",
            "|[0.80271,0.0,8.14...|20.2|\n",
            "|[0.7258,0.0,8.14,...|18.2|\n",
            "+--------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgFCto2wHLd"
      },
      "source": [
        "Тут \"Attributes\" містяться у вхідних функціях з усіх стовпців, а \"medv\" - цільовий стовпець.\n",
        "Далі слід розділити дані навчання та тестування відповідно до нашого набору даних (у цьому випадку 0,8 та 0,2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwe1VT0UNOIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682c8d44-2439-4049-b9f3-0205c57e2319"
      },
      "source": [
        "#Split training and testing data\n",
        "train_data,test_data = finalized_data.randomSplit([0.8,0.2])\n",
        "\n",
        "\n",
        "regressor = LinearRegression(featuresCol = 'Attributes', labelCol = 'medv')\n",
        "\n",
        "#Learn to fit the model from training set\n",
        "regressor = regressor.fit(train_data)\n",
        "\n",
        "#To predict the prices on testing set\n",
        "pred = regressor.evaluate(test_data)\n",
        "\n",
        "#Predict the model\n",
        "pred.predictions.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+------------------+\n",
            "|          Attributes|medv|        prediction|\n",
            "+--------------------+----+------------------+\n",
            "|[0.01301,35.0,1.5...|32.7|29.883108263387108|\n",
            "|[0.01709,90.0,2.0...|30.1|24.923785962928225|\n",
            "|[0.01778,95.0,1.4...|32.9|30.603828392399723|\n",
            "|[0.01951,17.5,1.3...|33.0|22.640768692576714|\n",
            "|[0.02729,0.0,7.07...|34.7|30.642635886097388|\n",
            "|[0.03049,55.0,3.7...|31.2|28.663917224354197|\n",
            "|[0.03427,0.0,5.19...|19.5| 20.33525190533025|\n",
            "|[0.03584,80.0,3.3...|23.5|30.670897995407657|\n",
            "|[0.03615,80.0,4.9...|27.9| 32.66282048539621|\n",
            "|[0.04301,80.0,1.9...|18.2|14.041332457416878|\n",
            "|[0.04337,21.0,5.6...|20.5|23.706177649907012|\n",
            "|[0.0459,52.5,5.32...|22.3|27.193972377903116|\n",
            "|[0.0536,21.0,5.64...|25.0|27.640560167868337|\n",
            "|[0.05425,0.0,4.05...|24.6|29.536830438792215|\n",
            "|[0.05479,33.0,2.1...|28.4|31.489242242531187|\n",
            "|[0.05561,70.0,2.2...|29.0|31.764503591806907|\n",
            "|[0.06127,40.0,6.4...|33.1| 34.99846973039373|\n",
            "|[0.06263,0.0,11.9...|22.4| 23.78437117684898|\n",
            "|[0.06417,0.0,5.96...|18.9| 24.21176439247398|\n",
            "|[0.07244,60.0,1.6...|18.6|16.120552947235716|\n",
            "+--------------------+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "Ми також можемо надрукувати коефіцієнт і перехоплення моделі регресії, використовуючи таку команду:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3f0449-90dc-4c04-f7c4-cbe1d956f060"
      },
      "source": [
        "#coefficient of the regression model\n",
        "coeff = regressor.coefficients\n",
        "\n",
        "#X and Y intercept\n",
        "intr = regressor.intercept\n",
        "\n",
        "print (\"The coefficient of the model is : %a\" %coeff)\n",
        "print (\"The Intercept of the model is : %f\" %intr)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of the model is : DenseVector([-0.1443, 0.0522, 0.0175, 2.3272, -18.5393, 3.6335, -0.0002, -1.6747, 0.3657, -0.0138, -0.8988, 0.0087, -0.5934])\n",
            "The Intercept of the model is : 39.020432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qrQdEj62ptt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633f0669-2466-4c6c-89e8-a1b6352e9902"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "eval = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Root Mean Square Error\n",
        "rmse = eval.evaluate(pred.predictions)\n",
        "print(\"RMSE: %.3f\" % rmse)\n",
        "\n",
        "# Mean Square Error\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "print(\"MSE: %.3f\" % mse)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "print(\"MAE: %.3f\" % mae)\n",
        "\n",
        "# r2 - coefficient of determination\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
        "print(\"r2: %.3f\" %r2)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 4.148\n",
            "MSE: 17.204\n",
            "MAE: 3.116\n",
            "r2: 0.750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcx7qOQbbtb"
      },
      "source": [
        "#Ще один приклад лінійної регрессії\n",
        "\n",
        "[Тут](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=278443377095&utm_targetid=dsa-473406582395&utm_loc_interest_ms=&utm_loc_physical_ms=9061015&gclid=CjwKCAjw47eFBhA9EiwAy8kzNGbswRR2ftFQ3G2QRV4sJaXG3gYumCFj3bHvXeHynV-rbUC590Z0VxoCGOcQAvD_BwE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXxb9RNyTfPv"
      },
      "source": [
        "#Створення Spark служби"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeP-sGnYRr-D"
      },
      "source": [
        "Згенеруємо службу та запустимо її"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orverCFWVwB7"
      },
      "source": [
        "sparkjob =  '''\n",
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        " \n",
        "  # create Spark context with Spark configuration\n",
        "  conf = SparkConf().setAppName(\"Read Text to RDD - Python\")\n",
        "  sc = SparkContext(conf=conf)\n",
        " \n",
        "  # read input text file to RDD\n",
        "  numbers = sc.parallelize([1,7,8,9,5,77,48])\n",
        " \n",
        "  # aggregate RDD elements using addition function\n",
        "  sum = numbers.reduce(lambda a,b:a+b)\n",
        " \n",
        "  print (\"sum is : \" + str(sum))\n",
        "'''\n",
        "\n",
        "f = open('testsparkjob.py', 'w')\n",
        "f.write(sparkjob)\n",
        "f.close()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcIpcpVQRbG_",
        "outputId": "b370c462-7fbe-4259-dfdb-7bf306c1d2ef"
      },
      "source": [
        "!/content/spark-3.1.1-bin-hadoop2.7/bin/spark-submit /content/testsparkjob.py"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/05/26 11:48:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/05/26 11:48:15 INFO SparkContext: Running Spark version 3.1.1\n",
            "21/05/26 11:48:15 INFO ResourceUtils: ==============================================================\n",
            "21/05/26 11:48:15 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/05/26 11:48:15 INFO ResourceUtils: ==============================================================\n",
            "21/05/26 11:48:15 INFO SparkContext: Submitted application: Read Text to RDD - Python\n",
            "21/05/26 11:48:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/05/26 11:48:16 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/05/26 11:48:16 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/05/26 11:48:16 INFO SecurityManager: Changing view acls to: root\n",
            "21/05/26 11:48:16 INFO SecurityManager: Changing modify acls to: root\n",
            "21/05/26 11:48:16 INFO SecurityManager: Changing view acls groups to: \n",
            "21/05/26 11:48:16 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/05/26 11:48:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/05/26 11:48:16 INFO Utils: Successfully started service 'sparkDriver' on port 46257.\n",
            "21/05/26 11:48:16 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/05/26 11:48:16 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/05/26 11:48:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/05/26 11:48:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/05/26 11:48:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/05/26 11:48:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3eb922be-827e-4268-8ace-affc4a9f71dd\n",
            "21/05/26 11:48:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "21/05/26 11:48:16 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/05/26 11:48:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "21/05/26 11:48:16 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "21/05/26 11:48:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a4936f36ae03:4041\n",
            "21/05/26 11:48:17 INFO Executor: Starting executor ID driver on host a4936f36ae03\n",
            "21/05/26 11:48:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41857.\n",
            "21/05/26 11:48:17 INFO NettyBlockTransferService: Server created on a4936f36ae03:41857\n",
            "21/05/26 11:48:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/05/26 11:48:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a4936f36ae03, 41857, None)\n",
            "21/05/26 11:48:17 INFO BlockManagerMasterEndpoint: Registering block manager a4936f36ae03:41857 with 366.3 MiB RAM, BlockManagerId(driver, a4936f36ae03, 41857, None)\n",
            "21/05/26 11:48:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a4936f36ae03, 41857, None)\n",
            "21/05/26 11:48:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a4936f36ae03, 41857, None)\n",
            "21/05/26 11:48:18 INFO SparkContext: Starting job: reduce at /content/testsparkjob.py:16\n",
            "21/05/26 11:48:18 INFO DAGScheduler: Got job 0 (reduce at /content/testsparkjob.py:16) with 2 output partitions\n",
            "21/05/26 11:48:18 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /content/testsparkjob.py:16)\n",
            "21/05/26 11:48:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "21/05/26 11:48:18 INFO DAGScheduler: Missing parents: List()\n",
            "21/05/26 11:48:18 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /content/testsparkjob.py:16), which has no missing parents\n",
            "21/05/26 11:48:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.3 KiB, free 366.3 MiB)\n",
            "21/05/26 11:48:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 366.3 MiB)\n",
            "21/05/26 11:48:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a4936f36ae03:41857 (size: 3.5 KiB, free: 366.3 MiB)\n",
            "21/05/26 11:48:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383\n",
            "21/05/26 11:48:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /content/testsparkjob.py:16) (first 15 tasks are for partitions Vector(0, 1))\n",
            "21/05/26 11:48:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "21/05/26 11:48:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (a4936f36ae03, executor driver, partition 0, PROCESS_LOCAL, 4493 bytes) taskResourceAssignments Map()\n",
            "21/05/26 11:48:18 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (a4936f36ae03, executor driver, partition 1, PROCESS_LOCAL, 4520 bytes) taskResourceAssignments Map()\n",
            "21/05/26 11:48:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "21/05/26 11:48:18 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "21/05/26 11:48:20 INFO PythonRunner: Times: total = 647, boot = 622, init = 25, finish = 0\n",
            "21/05/26 11:48:20 INFO PythonRunner: Times: total = 642, boot = 623, init = 19, finish = 0\n",
            "21/05/26 11:48:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1354 bytes result sent to driver\n",
            "21/05/26 11:48:20 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1354 bytes result sent to driver\n",
            "21/05/26 11:48:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1371 ms on a4936f36ae03 (executor driver) (1/2)\n",
            "21/05/26 11:48:20 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1338 ms on a4936f36ae03 (executor driver) (2/2)\n",
            "21/05/26 11:48:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "21/05/26 11:48:20 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41753\n",
            "21/05/26 11:48:20 INFO DAGScheduler: ResultStage 0 (reduce at /content/testsparkjob.py:16) finished in 1.682 s\n",
            "21/05/26 11:48:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "21/05/26 11:48:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "21/05/26 11:48:20 INFO DAGScheduler: Job 0 finished: reduce at /content/testsparkjob.py:16, took 1.797151 s\n",
            "sum is : 155\n",
            "21/05/26 11:48:20 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "21/05/26 11:48:20 INFO SparkUI: Stopped Spark web UI at http://a4936f36ae03:4041\n",
            "21/05/26 11:48:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "21/05/26 11:48:20 INFO MemoryStore: MemoryStore cleared\n",
            "21/05/26 11:48:20 INFO BlockManager: BlockManager stopped\n",
            "21/05/26 11:48:20 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "21/05/26 11:48:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "21/05/26 11:48:20 INFO SparkContext: Successfully stopped SparkContext\n",
            "21/05/26 11:48:20 INFO ShutdownHookManager: Shutdown hook called\n",
            "21/05/26 11:48:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7609b524-d703-4f94-b063-ea06e6f9f83d/pyspark-432d854a-d33d-4db5-9dcf-b0cc148e3fa3\n",
            "21/05/26 11:48:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7609b524-d703-4f94-b063-ea06e6f9f83d\n",
            "21/05/26 11:48:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-128bf4c7-9ecd-46b4-ba40-a89748627d06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4EwKqUnny6b"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}