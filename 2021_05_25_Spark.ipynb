{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_05_25_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrynaTkachenko/ai-experience/blob/main/2021_05_25_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "# Oснови Spark\n",
        "\n",
        "Для запуску в середовищі Google Colab запустимо наступний код"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e510bb-8e76-4ed7-f994-baf66ddfd594"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://apache.volia.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 11:21:01--  https://apache.volia.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
            "Resolving apache.volia.net (apache.volia.net)... 82.144.192.7\n",
            "Connecting to apache.volia.net (apache.volia.net)|82.144.192.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224374704 (214M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.1-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-3.1.1-bin-had 100%[===================>] 213.98M  14.9MB/s    in 14s     \n",
            "\n",
            "2021-05-26 11:21:17 (14.9 MB/s) - ‘spark-3.1.1-bin-hadoop2.7.tgz’ saved [224374704/224374704]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCItMgNZRJtE"
      },
      "source": [
        "!tar xf '/content/spark-3.1.1-bin-hadoop2.7.tgz'\n",
        "!pip install -q findspark"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Spark та Java у Colab встановлено.\n",
        "Встановимо шляхи до середовища, які дозволятимуть запускати Pyspark. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Запустимо локальну сесію Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkfvbP72BO24"
      },
      "source": [
        "Вітаємо! Colab вже працює з Pyspark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BldrdnAIBROK"
      },
      "source": [
        "#Spark Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X48FGLdxZq9G",
        "outputId": "524686ea-6cdc-4275-ac85-42be233fd588"
      },
      "source": [
        "valuesA = [('Pirate',1),('Monkey',2),('Ninja',3),('Spaghetti',4)]\n",
        "TableA = spark.createDataFrame(valuesA,['name','id'])\n",
        " \n",
        "valuesB = [('Rutabaga',1),('Pirate',2),('Ninja',3),('Darth Vader',4)]\n",
        "TableB = spark.createDataFrame(valuesB,['name','id'])\n",
        " \n",
        "TableA.show()\n",
        "TableB.show()\n",
        "\n",
        "ta = TableA.alias('ta')\n",
        "tb = TableB.alias('tb')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---+\n",
            "|     name| id|\n",
            "+---------+---+\n",
            "|   Pirate|  1|\n",
            "|   Monkey|  2|\n",
            "|    Ninja|  3|\n",
            "|Spaghetti|  4|\n",
            "+---------+---+\n",
            "\n",
            "+-----------+---+\n",
            "|       name| id|\n",
            "+-----------+---+\n",
            "|   Rutabaga|  1|\n",
            "|     Pirate|  2|\n",
            "|      Ninja|  3|\n",
            "|Darth Vader|  4|\n",
            "+-----------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcEqL20wCXuo"
      },
      "source": [
        "#Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYaifjh_aXQR",
        "outputId": "d74508ad-ae04-4770-c695-a2bc7b296ecd"
      },
      "source": [
        "inner_join = ta.join(tb, ta.name == tb.name)\n",
        "inner_join.show()\n",
        "\n",
        "left_join = ta.join(tb, ta.name == tb.name,how='left') \n",
        "left_join.show()\n",
        "\n",
        "right_join = ta.join(tb, ta.name == tb.name,how='right')\n",
        "right_join.show()\n",
        "\n",
        "full_outer_join = ta.join(tb, ta.name == tb.name,how='full')\n",
        "full_outer_join.show()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+------+---+\n",
            "|  name| id|  name| id|\n",
            "+------+---+------+---+\n",
            "| Ninja|  3| Ninja|  3|\n",
            "|Pirate|  1|Pirate|  2|\n",
            "+------+---+------+---+\n",
            "\n",
            "+---------+---+------+----+\n",
            "|     name| id|  name|  id|\n",
            "+---------+---+------+----+\n",
            "|Spaghetti|  4|  null|null|\n",
            "|    Ninja|  3| Ninja|   3|\n",
            "|   Pirate|  1|Pirate|   2|\n",
            "|   Monkey|  2|  null|null|\n",
            "+---------+---+------+----+\n",
            "\n",
            "+------+----+-----------+---+\n",
            "|  name|  id|       name| id|\n",
            "+------+----+-----------+---+\n",
            "|  null|null|   Rutabaga|  1|\n",
            "| Ninja|   3|      Ninja|  3|\n",
            "|Pirate|   1|     Pirate|  2|\n",
            "|  null|null|Darth Vader|  4|\n",
            "+------+----+-----------+---+\n",
            "\n",
            "+---------+----+-----------+----+\n",
            "|     name|  id|       name|  id|\n",
            "+---------+----+-----------+----+\n",
            "|     null|null|   Rutabaga|   1|\n",
            "|Spaghetti|   4|       null|null|\n",
            "|    Ninja|   3|      Ninja|   3|\n",
            "|   Pirate|   1|     Pirate|   2|\n",
            "|   Monkey|   2|       null|null|\n",
            "|     null|null|Darth Vader|   4|\n",
            "+---------+----+-----------+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "# Spark ML. Модель лінійної регресії\n",
        "\n",
        "\n",
        "Модель лінійної регресії є одним із найдавніших та широко застосовуваних підходів до машинного навчання, який передбачає взаємозв'язок між залежними та незалежними змінними. \n",
        "\n",
        "Лінійна регресія складається з лінії, що найкраще підходить, через розрізнені точки на графіку, а лінія, що найкраще підходить, відома як лінія регресії.\n",
        "\n",
        "Мета цієї вправи прогнозувати ціни на житло за заданими ознаками. Давайте спрогнозуємо ціни набору даних про житло в Бостоні, розглядаючи MEDV як вихідну змінну, а всі інші змінні [текст посилання](https://)як вхідні."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ee0470-3fa4-4354-bd0e-d1266e746d51"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/asifahmed90/pyspark-ML-in-Colab/master/BostonHousing.csv\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 11:21:35--  https://raw.githubusercontent.com/asifahmed90/pyspark-ML-in-Colab/master/BostonHousing.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35735 (35K) [text/plain]\n",
            "Saving to: ‘BostonHousing.csv’\n",
            "\n",
            "BostonHousing.csv   100%[===================>]  34.90K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-05-26 11:21:35 (23.0 MB/s) - ‘BostonHousing.csv’ saved [35735/35735]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg"
      },
      "source": [
        "Перевіряємо наявність файлів"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13e51af-b170-4cc8-b3b5-d549f4d3d2e0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BostonHousing.csv  spark-3.1.1-bin-hadoop2.7\n",
            "sample_data\t   spark-3.1.1-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Тепер, коли ми завантажили набір даних, ми можемо розпочати аналіз.\n",
        "Для нашої моделі лінійної регресії нам потрібно імпортувати два модулі з Pyspark, тобто Vector Assembler та Linear Regression. Vector Assembler - це трансформатор, який збирає всі функції в один вектор із декількох стовпців, що містять тип float/double. Ми могли б використовувати StringIndexer, якщо будь-який із наших стовпців містить значення рядків для перетворення його в числові значення. На щастя, набір даних BostonHousing містить лише подвійні значення, тому наразі нам не потрібно турбуватися про StringIndexer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import StructType, IntegerType, DateType, StringType, FloatType\n",
        "\n",
        "#'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'\n",
        "schema = StructType([\n",
        "    StructField(\"crim\", FloatType()),\n",
        "    StructField(\"zn\", FloatType()),\n",
        "    StructField(\"indus\", FloatType()),\n",
        "    StructField(\"chas\", FloatType()),\n",
        "    StructField(\"nox\", FloatType()),\n",
        "    StructField(\"rm\", FloatType()),\n",
        "    StructField(\"age\", FloatType()),\n",
        "    StructField(\"dis\", FloatType()),\n",
        "    StructField(\"rad\", FloatType()),\n",
        "    StructField(\"tax\", FloatType()),\n",
        "    StructField(\"ptratio\", FloatType()),\n",
        "    StructField(\"b\", FloatType()),\n",
        "    StructField(\"lstat\", FloatType()),\n",
        "    StructField(\"medv\", FloatType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.csv('BostonHousing.csv', schema=schema, header =True, \n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzHsmSOVXViW"
      },
      "source": [
        "# Або\n",
        "dataset = spark.read.csv('BostonHousing.csv', inferSchema=True, header =True, \n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnTARPleoCVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367c45b9-fe6e-4b9c-ac7e-a22a0cca05ea"
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.538, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, b=396.9, lstat=4.98, medv=24.0),\n",
              " Row(crim=0.02731, zn=0.0, indus=7.07, chas=0, nox=0.469, rm=6.421, age=78.9, dis=4.9671, rad=2, tax=242, ptratio=17.8, b=396.9, lstat=9.14, medv=21.6),\n",
              " Row(crim=0.02729, zn=0.0, indus=7.07, chas=0, nox=0.469, rm=7.185, age=61.1, dis=4.9671, rad=2, tax=242, ptratio=17.8, b=392.83, lstat=4.03, medv=34.7),\n",
              " Row(crim=0.03237, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=6.998, age=45.8, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=394.63, lstat=2.94, medv=33.4),\n",
              " Row(crim=0.06905, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=7.147, age=54.2, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=396.9, lstat=5.33, medv=36.2),\n",
              " Row(crim=0.02985, zn=0.0, indus=2.18, chas=0, nox=0.458, rm=6.43, age=58.7, dis=6.0622, rad=3, tax=222, ptratio=18.7, b=394.12, lstat=5.21, medv=28.7),\n",
              " Row(crim=0.08829, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.012, age=66.6, dis=5.5605, rad=5, tax=311, ptratio=15.2, b=395.6, lstat=12.43, medv=22.9),\n",
              " Row(crim=0.14455, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.172, age=96.1, dis=5.9505, rad=5, tax=311, ptratio=15.2, b=396.9, lstat=19.15, medv=27.1),\n",
              " Row(crim=0.21124, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=5.631, age=100.0, dis=6.0821, rad=5, tax=311, ptratio=15.2, b=386.63, lstat=29.93, medv=16.5),\n",
              " Row(crim=0.17004, zn=12.5, indus=7.87, chas=0, nox=0.524, rm=6.004, age=85.9, dis=6.5921, rad=5, tax=311, ptratio=15.2, b=386.71, lstat=17.1, medv=18.9)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLoAfqVv8-E"
      },
      "source": [
        "Зверніть увагу, що ми використовували InferSchema всередині модуля read.csv. InferSchema дозволяє нам автоматично виводити різні типи даних для кожного стовпця.\n",
        "\n",
        "Давайте роздрукуємося в наборі даних, щоб побачити типи даних кожного стовпця:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gok1FXWugYkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebf7493-f8bc-4c74-9a09-9b53e5f5ae99"
      },
      "source": [
        "dataset.printSchema()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- crim: double (nullable = true)\n",
            " |-- zn: double (nullable = true)\n",
            " |-- indus: double (nullable = true)\n",
            " |-- chas: integer (nullable = true)\n",
            " |-- nox: double (nullable = true)\n",
            " |-- rm: double (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- dis: double (nullable = true)\n",
            " |-- rad: integer (nullable = true)\n",
            " |-- tax: integer (nullable = true)\n",
            " |-- ptratio: double (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- lstat: double (nullable = true)\n",
            " |-- medv: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9VJqsHwEGf"
      },
      "source": [
        "Наступним кроком є перетворення всіх функцій з різних стовпців в один стовпець і давайте назвемо цей новий векторний стовпець як 'Attributes' у outputCol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSqdT9QgkfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba20df4a-4e33-4621-a62b-ca7c9f421978"
      },
      "source": [
        "#Input all the features in one vector column\n",
        "assembler = VectorAssembler(inputCols=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'], outputCol = 'Attributes')\n",
        "output = assembler.transform(dataset)\n",
        "#Input vs Output\n",
        "finalized_data = output.select(\"Attributes\",\"medv\")\n",
        "\n",
        "#print(finalized_data)\n",
        "finalized_data.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+\n",
            "|          Attributes|medv|\n",
            "+--------------------+----+\n",
            "|[0.00632,18.0,2.3...|24.0|\n",
            "|[0.02731,0.0,7.07...|21.6|\n",
            "|[0.02729,0.0,7.07...|34.7|\n",
            "|[0.03237,0.0,2.18...|33.4|\n",
            "|[0.06905,0.0,2.18...|36.2|\n",
            "|[0.02985,0.0,2.18...|28.7|\n",
            "|[0.08829,12.5,7.8...|22.9|\n",
            "|[0.14455,12.5,7.8...|27.1|\n",
            "|[0.21124,12.5,7.8...|16.5|\n",
            "|[0.17004,12.5,7.8...|18.9|\n",
            "|[0.22489,12.5,7.8...|15.0|\n",
            "|[0.11747,12.5,7.8...|18.9|\n",
            "|[0.09378,12.5,7.8...|21.7|\n",
            "|[0.62976,0.0,8.14...|20.4|\n",
            "|[0.63796,0.0,8.14...|18.2|\n",
            "|[0.62739,0.0,8.14...|19.9|\n",
            "|[1.05393,0.0,8.14...|23.1|\n",
            "|[0.7842,0.0,8.14,...|17.5|\n",
            "|[0.80271,0.0,8.14...|20.2|\n",
            "|[0.7258,0.0,8.14,...|18.2|\n",
            "+--------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgFCto2wHLd"
      },
      "source": [
        "Тут \"Attributes\" містяться у вхідних функціях з усіх стовпців, а \"medv\" - цільовий стовпець.\n",
        "Далі слід розділити дані навчання та тестування відповідно до нашого набору даних (у цьому випадку 0,8 та 0,2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwe1VT0UNOIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb423720-fb31-4b8b-ad02-5f8f80236b6f"
      },
      "source": [
        "#Split training and testing data\n",
        "train_data,test_data = finalized_data.randomSplit([0.8,0.2])\n",
        "\n",
        "\n",
        "regressor = LinearRegression(featuresCol = 'Attributes', labelCol = 'medv')\n",
        "\n",
        "#Learn to fit the model from training set\n",
        "regressor = regressor.fit(train_data)\n",
        "\n",
        "#To predict the prices on testing set\n",
        "pred = regressor.evaluate(test_data)\n",
        "\n",
        "#Predict the model\n",
        "pred.predictions.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+------------------+\n",
            "|          Attributes|medv|        prediction|\n",
            "+--------------------+----+------------------+\n",
            "|[0.01381,80.0,0.4...|50.0| 40.71956560538103|\n",
            "|[0.03113,0.0,4.39...|17.5| 16.70472739243855|\n",
            "|[0.0315,95.0,1.47...|34.9| 29.83602755233254|\n",
            "|[0.03237,0.0,2.18...|33.4| 28.35829749190973|\n",
            "|[0.03537,34.0,6.0...|22.0|29.018686554335837|\n",
            "|[0.03615,80.0,4.9...|27.9| 31.35078594732833|\n",
            "|[0.03738,0.0,5.19...|20.7| 21.45858679336739|\n",
            "|[0.03932,0.0,3.41...|22.0| 26.95720815526486|\n",
            "|[0.04294,28.0,15....|20.6|27.092292046887426|\n",
            "|[0.04379,80.0,3.3...|19.4| 25.47386654474775|\n",
            "|[0.04462,25.0,4.8...|23.9|26.827193941154068|\n",
            "|[0.04684,0.0,3.41...|22.6|26.668585559780865|\n",
            "|[0.05059,0.0,4.49...|23.9|24.607343931531595|\n",
            "|[0.05372,0.0,13.9...|27.1|27.715858395528386|\n",
            "|[0.05602,0.0,2.46...|50.0|35.492669116909425|\n",
            "|[0.05646,0.0,12.8...|21.2| 21.32639536545326|\n",
            "|[0.06588,0.0,2.46...|39.8| 34.47247728855473|\n",
            "|[0.07013,0.0,13.8...|28.7|28.548961897153035|\n",
            "|[0.07151,0.0,4.49...|22.2|25.492215401005033|\n",
            "|[0.07886,80.0,4.9...|37.3| 33.95103039959372|\n",
            "+--------------------+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "Ми також можемо надрукувати коефіцієнт і перехоплення моделі регресії, використовуючи таку команду:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a5cbd6-2d7d-421d-9f73-07d04f4bd0b7"
      },
      "source": [
        "#coefficient of the regression model\n",
        "coeff = regressor.coefficients\n",
        "\n",
        "#X and Y intercept\n",
        "intr = regressor.intercept\n",
        "\n",
        "print (\"The coefficient of the model is : %a\" %coeff)\n",
        "print (\"The Intercept of the model is : %f\" %intr)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of the model is : DenseVector([-0.1133, 0.0453, 0.0511, 2.8776, -17.5876, 3.9407, 0.0035, -1.3205, 0.3252, -0.0135, -1.0176, 0.0102, -0.4814])\n",
            "The Intercept of the model is : 35.017077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qrQdEj62ptt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45475b96-cc59-4d97-d0a3-5b458700d088"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "eval = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Root Mean Square Error\n",
        "rmse = eval.evaluate(pred.predictions)\n",
        "print(\"RMSE: %.3f\" % rmse)\n",
        "\n",
        "# Mean Square Error\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "print(\"MSE: %.3f\" % mse)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "print(\"MAE: %.3f\" % mae)\n",
        "\n",
        "# r2 - coefficient of determination\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
        "print(\"r2: %.3f\" %r2)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 5.339\n",
            "MSE: 28.504\n",
            "MAE: 3.675\n",
            "r2: 0.700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcx7qOQbbtb"
      },
      "source": [
        "#Ще один приклад лінійної регрессії\n",
        "\n",
        "[Тут](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=278443377095&utm_targetid=dsa-473406582395&utm_loc_interest_ms=&utm_loc_physical_ms=9061015&gclid=CjwKCAjw47eFBhA9EiwAy8kzNGbswRR2ftFQ3G2QRV4sJaXG3gYumCFj3bHvXeHynV-rbUC590Z0VxoCGOcQAvD_BwE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXxb9RNyTfPv"
      },
      "source": [
        "#Створення Spark служби"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeP-sGnYRr-D"
      },
      "source": [
        "Згенеруємо службу та запустимо її"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orverCFWVwB7"
      },
      "source": [
        "sparkjob =  '''\n",
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        " \n",
        "  # create Spark context with Spark configuration\n",
        "  conf = SparkConf().setAppName(\"Read Text to RDD - Python\")\n",
        "  sc = SparkContext(conf=conf)\n",
        " \n",
        "  # read input text file to RDD\n",
        "  numbers = sc.parallelize([1,7,8,9,5,77,48])\n",
        " \n",
        "  # aggregate RDD elements using addition function\n",
        "  sum = numbers.reduce(lambda a,b:a+b)\n",
        " \n",
        "  print (\"sum is : \" + str(sum))\n",
        "'''\n",
        "\n",
        "f = open('testsparkjob.py', 'w')\n",
        "f.write(sparkjob)\n",
        "f.close()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcIpcpVQRbG_",
        "outputId": "0f408d2e-3180-4e99-fc5b-06b6d0b2bc75"
      },
      "source": [
        "!/content/spark-3.1.1-bin-hadoop2.7/bin/spark-submit /content/testsparkjob.py"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/05/26 11:21:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/05/26 11:21:44 INFO SparkContext: Running Spark version 3.1.1\n",
            "21/05/26 11:21:45 INFO ResourceUtils: ==============================================================\n",
            "21/05/26 11:21:45 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/05/26 11:21:45 INFO ResourceUtils: ==============================================================\n",
            "21/05/26 11:21:45 INFO SparkContext: Submitted application: Read Text to RDD - Python\n",
            "21/05/26 11:21:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/05/26 11:21:45 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/05/26 11:21:45 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/05/26 11:21:45 INFO SecurityManager: Changing view acls to: root\n",
            "21/05/26 11:21:45 INFO SecurityManager: Changing modify acls to: root\n",
            "21/05/26 11:21:45 INFO SecurityManager: Changing view acls groups to: \n",
            "21/05/26 11:21:45 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/05/26 11:21:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/05/26 11:21:45 INFO Utils: Successfully started service 'sparkDriver' on port 36409.\n",
            "21/05/26 11:21:45 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/05/26 11:21:45 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/05/26 11:21:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/05/26 11:21:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/05/26 11:21:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/05/26 11:21:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e616061-ac1e-42d9-a8d5-02a8676095db\n",
            "21/05/26 11:21:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "21/05/26 11:21:45 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/05/26 11:21:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "21/05/26 11:21:46 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "21/05/26 11:21:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a4936f36ae03:4041\n",
            "21/05/26 11:21:46 INFO Executor: Starting executor ID driver on host a4936f36ae03\n",
            "21/05/26 11:21:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44483.\n",
            "21/05/26 11:21:46 INFO NettyBlockTransferService: Server created on a4936f36ae03:44483\n",
            "21/05/26 11:21:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/05/26 11:21:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a4936f36ae03, 44483, None)\n",
            "21/05/26 11:21:46 INFO BlockManagerMasterEndpoint: Registering block manager a4936f36ae03:44483 with 366.3 MiB RAM, BlockManagerId(driver, a4936f36ae03, 44483, None)\n",
            "21/05/26 11:21:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a4936f36ae03, 44483, None)\n",
            "21/05/26 11:21:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a4936f36ae03, 44483, None)\n",
            "21/05/26 11:21:47 INFO SparkContext: Starting job: reduce at /content/testsparkjob.py:16\n",
            "21/05/26 11:21:47 INFO DAGScheduler: Got job 0 (reduce at /content/testsparkjob.py:16) with 2 output partitions\n",
            "21/05/26 11:21:47 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /content/testsparkjob.py:16)\n",
            "21/05/26 11:21:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "21/05/26 11:21:47 INFO DAGScheduler: Missing parents: List()\n",
            "21/05/26 11:21:47 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /content/testsparkjob.py:16), which has no missing parents\n",
            "21/05/26 11:21:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.3 KiB, free 366.3 MiB)\n",
            "21/05/26 11:21:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 366.3 MiB)\n",
            "21/05/26 11:21:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a4936f36ae03:44483 (size: 3.5 KiB, free: 366.3 MiB)\n",
            "21/05/26 11:21:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383\n",
            "21/05/26 11:21:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /content/testsparkjob.py:16) (first 15 tasks are for partitions Vector(0, 1))\n",
            "21/05/26 11:21:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "21/05/26 11:21:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (a4936f36ae03, executor driver, partition 0, PROCESS_LOCAL, 4493 bytes) taskResourceAssignments Map()\n",
            "21/05/26 11:21:47 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (a4936f36ae03, executor driver, partition 1, PROCESS_LOCAL, 4520 bytes) taskResourceAssignments Map()\n",
            "21/05/26 11:21:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "21/05/26 11:21:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "21/05/26 11:21:49 INFO PythonRunner: Times: total = 698, boot = 671, init = 27, finish = 0\n",
            "21/05/26 11:21:49 INFO PythonRunner: Times: total = 716, boot = 678, init = 38, finish = 0\n",
            "21/05/26 11:21:49 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1354 bytes result sent to driver\n",
            "21/05/26 11:21:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1354 bytes result sent to driver\n",
            "21/05/26 11:21:49 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1381 ms on a4936f36ae03 (executor driver) (1/2)\n",
            "21/05/26 11:21:49 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36655\n",
            "21/05/26 11:21:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1442 ms on a4936f36ae03 (executor driver) (2/2)\n",
            "21/05/26 11:21:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "21/05/26 11:21:49 INFO DAGScheduler: ResultStage 0 (reduce at /content/testsparkjob.py:16) finished in 1.735 s\n",
            "21/05/26 11:21:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "21/05/26 11:21:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "21/05/26 11:21:49 INFO DAGScheduler: Job 0 finished: reduce at /content/testsparkjob.py:16, took 1.815376 s\n",
            "sum is : 155\n",
            "21/05/26 11:21:49 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "21/05/26 11:21:49 INFO SparkUI: Stopped Spark web UI at http://a4936f36ae03:4041\n",
            "21/05/26 11:21:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "21/05/26 11:21:49 INFO MemoryStore: MemoryStore cleared\n",
            "21/05/26 11:21:49 INFO BlockManager: BlockManager stopped\n",
            "21/05/26 11:21:49 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "21/05/26 11:21:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "21/05/26 11:21:49 INFO SparkContext: Successfully stopped SparkContext\n",
            "21/05/26 11:21:49 INFO ShutdownHookManager: Shutdown hook called\n",
            "21/05/26 11:21:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8c1e904-2b69-40ed-98b6-f6967bd074fe\n",
            "21/05/26 11:21:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8c1e904-2b69-40ed-98b6-f6967bd074fe/pyspark-8c8b4670-3f94-41ec-b376-2e728ac1425c\n",
            "21/05/26 11:21:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-29afbced-3615-4050-af78-f3ca8b8c9820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4EwKqUnny6b"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}